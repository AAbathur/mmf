(window.webpackJsonp=window.webpackJsonp||[]).push([[9],{111:function(e,t,n){"use strict";n.r(t),n.d(t,"frontMatter",(function(){return i})),n.d(t,"metadata",(function(){return l})),n.d(t,"rightToc",(function(){return c})),n.d(t,"default",(function(){return p}));var a=n(2),r=n(6),o=(n(0),n(124)),i={id:"pretrained_models",title:"Pretrained Models",sidebar_label:"Pretrained Models"},l={id:"notes/pretrained_models",title:"Pretrained Models",description:"[Outdated] A new version of this will be uploaded soon",source:"@site/docs/notes/pretrained_models.md",permalink:"/docs/notes/pretrained_models",editUrl:"https://github.com/facebookresearch/mmf/edit/master/website/docs/notes/pretrained_models.md",lastUpdatedBy:"Amanpreet Singh",lastUpdatedAt:1591757356,sidebar_label:"Pretrained Models",sidebar:"docs",previous:{title:"MMF's Configuration System",permalink:"/docs/notes/configuration"},next:{title:"Adding a dataset",permalink:"/docs/tutorials/dataset"}},c=[],s={rightToc:c};function p(e){var t=e.components,n=Object(r.a)(e,["components"]);return Object(o.b)("wrapper",Object(a.a)({},s,n,{components:t,mdxType:"MDXLayout"}),Object(o.b)("p",null,Object(o.b)("strong",{parentName:"p"},"[Outdated]")," A new version of this will be uploaded soon"),Object(o.b)("p",null,"Performing inference using pretrained models in MMF is easy. Pickup a pretrained\nmodel from the table below and follow the steps to do inference or generate\npredictions for EvalAI evaluation. This section expects that you have already installed the\nrequired data as explained in ",Object(o.b)("a",Object(a.a)({parentName:"p"},{href:"./quickstart"}),"quickstart"),"."),Object(o.b)("pre",null,Object(o.b)("code",Object(a.a)({parentName:"pre"},{className:"language-eval_rst"}),"+--------+-----------+---------------------------------------+---------------------------------------------------+-----------------------------------------------------------+\n| Model  | Model Key | Supported Datasets                    | Pretrained Models                                 | Notes                                                     |\n+--------+-----------+---------------------------------------+---------------------------------------------------+-----------------------------------------------------------+\n| Pythia | pythia    | vqa2, vizwiz, textvqa, visual_genome, | `vqa2 train+val`_, `vqa2 train only`_,  `vizwiz`_ | VizWiz model has been pretrained on VQAv2 and transferred |\n+--------+-----------+---------------------------------------+---------------------------------------------------+-----------------------------------------------------------+\n| LoRRA  | lorra     | vqa2, vizwiz, textvqa                 | `textvqa`_                                        |                                                           |\n+--------+-----------+---------------------------------------+---------------------------------------------------+-----------------------------------------------------------+\n| CNNLSTM| cnn_lstm  | clevr                                 |                                                   | Features are calculated on fly in this on                 |\n+--------+-----------+---------------------------------------+---------------------------------------------------+-----------------------------------------------------------+\n| BAN    | ban       | vqa2, vizwiz, textvqa                 | Coming soon!                                      | Support is preliminary and haven't been tested throughly. |\n+--------+-----------+---------------------------------------+---------------------------------------------------+-----------------------------------------------------------+\n| BUTD   | butd      | coco                                  | `coco`_                                           |                                                           |\n+--------+-----------+---------------------------------------+---------------------------------------------------+-----------------------------------------------------------+\n\n.. _vqa2 train+val: https://dl.fbaipublicfiles.com/pythia/pretrained_models/vqa2/pythia_train_val.pth\n.. _vqa2 train only: https://dl.fbaipublicfiles.com/pythia/pretrained_models/vqa2/pythia.pth\n.. _vizwiz: https://dl.fbaipublicfiles.com/pythia/pretrained_models/vizwiz/pythia_pretrained_vqa2.pth\n.. _textvqa: https://dl.fbaipublicfiles.com/pythia/pretrained_models/textvqa/lorra_best.pth\n.. _coco: https://dl.fbaipublicfiles.com/pythia/pretrained_models/coco_captions/butd.pth\n")),Object(o.b)("p",null,"Now, let's say your link to pretrained model ",Object(o.b)("inlineCode",{parentName:"p"},"model")," is ",Object(o.b)("inlineCode",{parentName:"p"},"link")," (select from table > right click > copy link address), the respective config should be at\n",Object(o.b)("inlineCode",{parentName:"p"},"configs/[task]/[dataset]/[model].yaml"),". For example, config file for ",Object(o.b)("inlineCode",{parentName:"p"},"vqa2 train_and_val")," should be\n",Object(o.b)("inlineCode",{parentName:"p"},"configs/vqa/vqa2/pythia_train_and_val.yaml"),". Now to run inference for EvalAI, run the following command."),Object(o.b)("pre",null,Object(o.b)("code",Object(a.a)({parentName:"pre"},{}),"cd ~/mmf/data\nmkdir -p models && cd models;\n# Download the pretrained model\nwget [link]\ncd ../..;\npython tools/run.py --datasets [dataset] --model [model] --config [config] \\\n--run_type inference --evalai_inference 1 --resume_file data/[model].pth\n")),Object(o.b)("p",null,"If you want to train or evaluate on val, change the ",Object(o.b)("inlineCode",{parentName:"p"},"run_type")," to ",Object(o.b)("inlineCode",{parentName:"p"},"train")," or ",Object(o.b)("inlineCode",{parentName:"p"},"val"),"\naccordingly. You can also use multiple run types, for e.g. to do training, inference on\nval as well as test you can set ",Object(o.b)("inlineCode",{parentName:"p"},"--run_type")," to ",Object(o.b)("inlineCode",{parentName:"p"},"train+val+inference"),"."),Object(o.b)("p",null,"if you remove ",Object(o.b)("inlineCode",{parentName:"p"},"--evalai_inference")," argument, Pythia will perform inference and provide results\ndirectly on the dataset. Do note that this is not possible in case of test sets as we\ndon't have answers/targets for them. So, this can be useful for performing inference\non val set locally."),Object(o.b)("p",null,"Table below shows evaluation metrics for various pretrained models:"),Object(o.b)("pre",null,Object(o.b)("code",Object(a.a)({parentName:"pre"},{className:"language-eval_rst"}),"+--------+--------------------+---------------------------------+----------------------------------------------+\n| Model  | Dataset            | Metric                          | Notes                                        |\n+--------+--------------------+---------------------------------+----------------------------------------------+\n| Pythia | vqa2 (train+val)   | test-dev accuracy - 68.31%      | Can be easily pushed to 69.2%                |\n+--------+--------------------+---------------------------------+----------------------------------------------+\n| Pythia | vqa2 (train)       | test-dev accuracy - 66.7%       |                                              |\n+--------+--------------------+---------------------------------+----------------------------------------------+\n| Pythia | vizwiz (train)     | test-dev accuracy - 54.22%      | Pretrained on VQA2 and transferred to VizWiz |\n+--------+--------------------+---------------------------------+----------------------------------------------+\n| LoRRA  | textvqa (train)    | val accuracy - 27.4%            |                                              |\n+--------+--------------------+---------------------------------+----------------------------------------------+\n|        |  coco              | | BLEU 1 - 76.02, BLEU 4- 35.42 |                                              |\n| BUTD   |  (karpathy-train)  | | METEOR- 27.39, ROUGE_L- 56.17 | Beam Search(length 5), Karpathy test split   |\n|        |                    | | CIDEr - 112.03, SPICE - 20.33 |                                              |\n+--------+--------------------+---------------------------------+----------------------------------------------+\n")),Object(o.b)("p",null,Object(o.b)("strong",{parentName:"p"},"Note for BUTD model :"),"  For training BUTD model use the config ",Object(o.b)("inlineCode",{parentName:"p"},"butd.yaml"),". Training uses greedy decoding for validation. Currently we do not have support to train the model using beam search decoding validation. We will add that support soon. For inference only use ",Object(o.b)("inlineCode",{parentName:"p"},"butd_beam_search.yaml")," config that supports beam search decoding."),Object(o.b)("p",null,Object(o.b)("strong",{parentName:"p"},"Note")," that, for simplicity, our current released model ",Object(o.b)("strong",{parentName:"p"},"does not")," incorporate extensive data augmentations (e.g. visual genome, visual dialogue) during training, which was used in our challenge winner entries for VQA and VizWiz 2018. As a result, there can be some performance gap to models reported and released previously. If you are looking for reproducing those results, please checkout the ",Object(o.b)("a",Object(a.a)({parentName:"p"},{href:"https://github.com/facebookresearch/mmf/releases/tag/v0.1"}),"v0.1")," release."))}p.isMDXComponent=!0},124:function(e,t,n){"use strict";n.d(t,"a",(function(){return d})),n.d(t,"b",(function(){return m}));var a=n(0),r=n.n(a);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function c(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=r.a.createContext({}),p=function(e){var t=r.a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},d=function(e){var t=p(e.components);return r.a.createElement(s.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.a.createElement(r.a.Fragment,{},t)}},b=r.a.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,i=e.parentName,s=c(e,["components","mdxType","originalType","parentName"]),d=p(n),b=a,m=d["".concat(i,".").concat(b)]||d[b]||u[b]||o;return n?r.a.createElement(m,l(l({ref:t},s),{},{components:n})):r.a.createElement(m,l({ref:t},s))}));function m(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,i=new Array(o);i[0]=b;var l={};for(var c in t)hasOwnProperty.call(t,c)&&(l[c]=t[c]);l.originalType=e,l.mdxType="string"==typeof e?e:a,i[1]=l;for(var s=2;s<o;s++)i[s]=n[s];return r.a.createElement.apply(null,i)}return r.a.createElement.apply(null,n)}b.displayName="MDXCreateElement"}}]);