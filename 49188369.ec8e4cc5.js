(window.webpackJsonp=window.webpackJsonp||[]).push([[11],{147:function(e,t,a){"use strict";a.r(t),a.d(t,"frontMatter",(function(){return o})),a.d(t,"metadata",(function(){return b})),a.d(t,"rightToc",(function(){return s})),a.d(t,"default",(function(){return c}));var r=a(2),n=a(9),i=(a(0),a(164)),o={id:"dataset_zoo",title:"Dataset Zoo",sidebar_label:"Dataset Zoo"},b={id:"notes/dataset_zoo",title:"Dataset Zoo",description:"Here is the current list of datasets currently supported in MMF:",source:"@site/docs/notes/dataset_zoo.md",permalink:"/docs/notes/dataset_zoo",editUrl:"https://github.com/facebookresearch/mmf/edit/master/website/docs/notes/dataset_zoo.md",lastUpdatedBy:"Vedanuj Goswami",lastUpdatedAt:1592293726,sidebar_label:"Dataset Zoo",sidebar:"docs",previous:{title:"MMF's Configuration System",permalink:"/docs/notes/configuration"},next:{title:"Model Zoo",permalink:"/docs/notes/model_zoo"}},s=[],l={rightToc:s};function c(e){var t=e.components,a=Object(n.a)(e,["components"]);return Object(i.b)("wrapper",Object(r.a)({},l,a,{components:t,mdxType:"MDXLayout"}),Object(i.b)("p",null,"Here is the current list of datasets currently supported in MMF:"),Object(i.b)("ul",null,Object(i.b)("li",{parentName:"ul"},Object(i.b)("strong",{parentName:"li"},"CLEVR")," ","[Paper : CLEVR A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning]",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://arxiv.org/abs/1612.06890"}),"arXiv"),"]"),Object(i.b)("li",{parentName:"ul"},Object(i.b)("strong",{parentName:"li"},"COCO Captions")," ","[Paper : Microsoft COCO Captions: Data Collection and Evaluation Server]",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://arxiv.org/abs/1504.00325"}),"arXiv"),"]"),Object(i.b)("li",{parentName:"ul"},Object(i.b)("strong",{parentName:"li"},"Conceptual Captions")," ","[Paper : Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning]",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://ai.google.com/research/ConceptualCaptions"}),"website"),"]"),Object(i.b)("li",{parentName:"ul"},Object(i.b)("strong",{parentName:"li"},"Hateful Memes")," ","[Paper : The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes]",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://arxiv.org/abs/2005.04790"}),"arXiv"),"]"),Object(i.b)("li",{parentName:"ul"},Object(i.b)("strong",{parentName:"li"},"MM IMDB")," ","[Paper : Gated Multimodal Units for Information Fusion]",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://arxiv.org/abs/1702.01992"}),"arXiv"),"] [",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"http://lisi1.unal.edu.co/mmimdb"}),"website"),"]"),Object(i.b)("li",{parentName:"ul"},Object(i.b)("strong",{parentName:"li"},"NLVR2")," ","[Paper : A Corpus for Reasoning About Natural Language Grounded in Photographs]",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://arxiv.org/abs/1811.00491"}),"arXiv"),"] [",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"http://lil.nlp.cornell.edu/nlvr/"}),"website"),"]"),Object(i.b)("li",{parentName:"ul"},Object(i.b)("strong",{parentName:"li"},"OCRVQA")," ","[Paper : OCR-VQA: Visual Question Answering by Reading Text in Images]",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://ocr-vqa.github.io/"}),"website"),"]"),Object(i.b)("li",{parentName:"ul"},Object(i.b)("strong",{parentName:"li"},"STVQA")," ","[Paper : Scene Text Visual Question Answering]",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://arxiv.org/abs/1905.13648"}),"arXiV"),"]"),Object(i.b)("li",{parentName:"ul"},Object(i.b)("strong",{parentName:"li"},"TextVQA")," ","[Paper : Towards VQA Models That Can Read]",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://arxiv.org/abs/1904.08920"}),"arXiV"),"] [",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://textvqa.org/"}),"website"),"]"),Object(i.b)("li",{parentName:"ul"},Object(i.b)("strong",{parentName:"li"},"TextCaps")," ","[Paper : TextCaps: a Dataset for Image Captioning with Reading Comprehension]",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://arxiv.org/abs/2003.12462"}),"arXiV"),"]"),Object(i.b)("li",{parentName:"ul"},Object(i.b)("strong",{parentName:"li"},"Visual Dialog")," ","[Paper : Visual Dialog]",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://visualdialog.org/"}),"website"),"]"),Object(i.b)("li",{parentName:"ul"},Object(i.b)("strong",{parentName:"li"},"SNLI-VE")," ","[Paper : Visual Entailment: A Novel Task for Fine-Grained Image Understanding]",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://arxiv.org/abs/1901.06706"}),"arXiV"),"] [",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://github.com/necla-ml/SNLI-VE"}),"website"),"]"),Object(i.b)("li",{parentName:"ul"},Object(i.b)("strong",{parentName:"li"},"Visual Genome")," ","[Paper : Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations]",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://arxiv.org/abs/1602.07332"}),"arXiV"),"] [",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://visualgenome.org/"}),"website"),"]"),Object(i.b)("li",{parentName:"ul"},Object(i.b)("strong",{parentName:"li"},"VizWiZ")," ","[Paper : VizWiz Grand Challenge: Answering Visual Questions from Blind People]",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://arxiv.org/abs/1802.08218"}),"arXiV"),"] [",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://vizwiz.org/"}),"website"),"]"),Object(i.b)("li",{parentName:"ul"},Object(i.b)("strong",{parentName:"li"},"VQA2.0")," ","[Paper : VQA: Visual Question Answering]",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://arxiv.org/abs/1505.00468"}),"arXiV"),"] [",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://visualqa.org/"}),"website"),"]")))}c.isMDXComponent=!0},164:function(e,t,a){"use strict";a.d(t,"a",(function(){return p})),a.d(t,"b",(function(){return O}));var r=a(0),n=a.n(r);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function b(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,r,n=function(e,t){if(null==e)return{};var a,r,n={},i=Object.keys(e);for(r=0;r<i.length;r++)a=i[r],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)a=i[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var l=n.a.createContext({}),c=function(e){var t=n.a.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):b(b({},t),e)),a},p=function(e){var t=c(e.components);return n.a.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.a.createElement(n.a.Fragment,{},t)}},m=n.a.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,o=e.parentName,l=s(e,["components","mdxType","originalType","parentName"]),p=c(a),m=r,O=p["".concat(o,".").concat(m)]||p[m]||u[m]||i;return a?n.a.createElement(O,b(b({ref:t},l),{},{components:a})):n.a.createElement(O,b({ref:t},l))}));function O(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=m;var b={};for(var s in t)hasOwnProperty.call(t,s)&&(b[s]=t[s]);b.originalType=e,b.mdxType="string"==typeof e?e:r,o[1]=b;for(var l=2;l<i;l++)o[l]=a[l];return n.a.createElement.apply(null,o)}return n.a.createElement.apply(null,a)}m.displayName="MDXCreateElement"}}]);